{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b7ba7b-433c-463c-8e5e-8b975a5be463",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Penguins in Production\n",
    "\n",
    "This notebook creates a [SageMaker Pipeline](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html) to build an end-to-end Machine Learning system to solve the problem of classifying penguin species. With a SageMaker Pipeline, you can create, automate, and manage end-to-end Machine Learning workflows at scale.\n",
    "\n",
    "You can find more information about Amazon SageMaker in the [Amazon SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html). The [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/) is an excellent source to stay up-to-date with SageMaker.\n",
    "\n",
    "This example uses the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data), the [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) library, and the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/). \n",
    "\n",
    "<img src='https://imgur.com/orZWHly.png' alt='Penguins dataset' width=\"800\">\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Session 1 - Production Machine Learning is Different](#Session-1---Production-Machine-Learning-is-Different) \n",
    "2. [Session 2 - Implementing a Production Pipeline](#Session-2---Implementing-a-Production-Pipeline)\n",
    "3. [Session 3 - Building a Model](#Session-3---Building-a-Model)\n",
    "4. [Session 4 - Evaluating and Registering The Model](#Session-4---Evaluating-and-Registering-The-Model)\n",
    "5. [Session 5 - Deploying The Model](#Session-5---Deploying-The-Model)\n",
    "6. [Session 6 - Monitoring](#Session-6---Monitoring)\n",
    "7. [Running the Pipeline](#Running-the-Pipeline)\n",
    "\n",
    "This notebook is part of the [Machine Learning School](https://www.ml.school) program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c712cfc-5ac2-481f-8019-815e97bfeca2",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#6e420c; color: #fff\"><strong>Note:</strong> \n",
    "    Make sure you run the <strong><a href=\"penguins-setup.ipynb\" style=\"color: #0397a7\">Setup notebook</a></strong> once at the start of the program and before running this notebook. After doing that, ensure you are using a \"TensorFlow 2.6 Python 3.8 CPU Optimized\" image and the \"packages\" start-up script to run this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "212a0538-62b5-4875-a59c-1a4e1a833a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "CODE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(f\"./{CODE_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11137928-6b4e-465c-8ad7-2297afbaa33c",
   "metadata": {},
   "source": [
    "# Session 1 - Production Machine Learning is Different\n",
    "\n",
    "In this session we'll run Exploratory Data Analysis on the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data). We'll split and transform the data using a [Scikit-Learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "545fa94c-9e20-446c-b769-2427032bbcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import sagemaker\n",
    "import sklearn\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# By default, The SageMaker SDK logs events related to the default\n",
    "# configuration using the INFO level. To prevent these from spoiling\n",
    "# the output of this notebook cells, we can change the logging\n",
    "# level to ERROR instead.\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fc999-1c6d-46ba-ab3a-0fdf819536ca",
   "metadata": {},
   "source": [
    "## Step 1 - Upload Dataset to S3\n",
    "\n",
    "Let's create the S3 bucket where we will organize every resource we are going to use during the program.\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#6e420c; color: #fff\"><strong>Note:</strong> \n",
    "    You need to select a unique name for the <strong>BUCKET</strong> constant. If you want to create a bucket in a region other than <strong>us-east-1</strong>, you need to use the \"--create-bucket-configuration\" argument when creating the bucket. You can see an example below.\n",
    "</div>\n",
    "\n",
    "Example of how to specify a region different from `us-east-1` when creating a bucket:\n",
    "\n",
    "```\n",
    "!aws s3api create-bucket --bucket $BUCKET --create-bucket-configuration LocationConstraint=\"eu-west-1\"\n",
    "```\n",
    "\n",
    "The `LocationConstraint` argument should specify the region where you want to create the bucket. The example above creates the bucket in the `eu-west-1` region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "806bcb29-2999-4126-90dd-0c929a68c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Location\": \"/mlschool\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "BUCKET = \"mlschool\"\n",
    "\n",
    "!aws s3api create-bucket --bucket $BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c6d73-cadb-42b9-b126-e73c485ec58e",
   "metadata": {},
   "source": [
    "After we have a bucket, we can download the [Penguins dataset](https://www.kaggle.com/parulpandey/palmer-archipelago-antarctica-penguin-data) and store it in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b03948f-0eba-440f-b439-2b9aa2b87c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://mlschool/penguins/data.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_LOCATION = f\"s3://{BUCKET}/penguins\"\n",
    "DATA_FILEPATH = Path().resolve() / \"data.csv\"\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv\", \n",
    "    DATA_FILEPATH\n",
    ")\n",
    "\n",
    "S3Uploader.upload(local_path=str(DATA_FILEPATH), desired_s3_uri=S3_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d2942-ffe5-42f2-994b-a11de97c43bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2 - Analyzing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a835695-557b-46d8-a901-a29bc57df5fe",
   "metadata": {},
   "source": [
    "Let's load the Penguins dataset. \n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "1. `species`: The species of a penguin. This is the column we want to predict.\n",
    "2. `island`: The island where the penguin was found\n",
    "3. `culmen_length_mm`: The length of the penguin's culmen (bill) in millimeters\n",
    "4. `culmen_depth_mm`: The depth of the penguin's culmen in millimeters\n",
    "5. `flipper_length_mm`: The length of the penguin's flipper in millimeters\n",
    "6. `body_mass_g`: The body mass of the penguin in grams\n",
    "7. `sex`: The sex of the penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1cd2f0e-446d-48a9-a008-b4f1cc593bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# penguins = pd.read_csv(DATA_FILEPATH / \"data.csv\")\n",
    "# penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eae10e-20c4-477e-b6b8-965c3a53566e",
   "metadata": {},
   "source": [
    "Now, let's get the summary statistics for the features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2107c25-e730-4e22-a1b8-5bda53e61124",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'penguins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-feb0a0f9e87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpenguins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'penguins' is not defined"
     ]
    }
   ],
   "source": [
    "# penguins.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e19af7-9f0f-45fe-b7d3-f19721c02a2b",
   "metadata": {},
   "source": [
    "The distribution of the categories in our dataset are:\n",
    "\n",
    "- `species`: There are 3 species of penguins in the dataset: Adelie (152), Gentoo (124), and Chinstrap (68).\n",
    "- `island`: Penguins are from 3 islands: Biscoe (168), Dream (124), and Torgersen (52).\n",
    "- `sex`: We have 168 male penguins, 165 female penguins, and 1 penguin with an ambiguous gender ('.')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1242122a-726e-4c37-a718-dd8e873d1612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'penguins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-96322a33a2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspecies_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenguins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0misland_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenguins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'island'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msex_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenguins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'penguins' is not defined"
     ]
    }
   ],
   "source": [
    "# species_distribution = penguins['species'].value_counts()\n",
    "# island_distribution = penguins['island'].value_counts()\n",
    "# sex_distribution = penguins['sex'].value_counts()\n",
    "\n",
    "# print(species_distribution)\n",
    "# print()\n",
    "# print(island_distribution)\n",
    "# print()\n",
    "# print(sex_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d98fdd-3b8c-40a2-b8dc-15162b4049e2",
   "metadata": {},
   "source": [
    "Let's replace the ambiguous value in the `sex` column with a null value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e0b69-cc7f-4206-b814-0186c4d9c414",
   "metadata": {},
   "source": [
    "## Step 3 - Building a Scikit-Learn Pipeline\n",
    "\n",
    "Let's create a Scikit-Learn pipeline to transform the dataset. Here are the transformations that will happen when we use this pipeline:\n",
    "\n",
    "* Numeric columns: First, we will impute any missing values using the mean of the column. Then, we will scale the values using a standard scaler.\n",
    "* Categorical columns: First, we will impute any missing values using the value that shows up more frequently in the data. Then, we will one-hot encode the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec808194-0d10-4717-b81e-2ee2302158c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numeric_features = penguins.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "# numeric_transformer = sklearn.pipeline.Pipeline(\n",
    "#     steps=[\n",
    "#         (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "#         (\"scaler\", StandardScaler()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# categorical_transformer = sklearn.pipeline.Pipeline(\n",
    "#     steps=[\n",
    "#         (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#         (\"encoder\", OneHotEncoder()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# target_transformer = sklearn.pipeline.Pipeline(\n",
    "#     steps=[\n",
    "#         # (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#         (\"encoder\", LabelEncoder()),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         # (\"t\", target_transformer, [\"species\"]),\n",
    "#         (\"numeric\", numeric_transformer, numeric_features),\n",
    "#         (\"categorical\", categorical_transformer, [\"island\"]),        \n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# pipeline = sklearn.pipeline.Pipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessing\", preprocessor)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62ceb064-6211-40b5-9ebe-347a63239a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# penguins.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ed57f-5b7d-4cf6-a044-0ac9a12df8f0",
   "metadata": {},
   "source": [
    "We can display a diagram of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ce20a20-d5ae-45b4-b277-586a7e357b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_config(display=\"diagram\")\n",
    "# pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06dc897-fe3a-4912-b83c-930ea02953fd",
   "metadata": {},
   "source": [
    "## Step 4 - Splitting and Transforming the Dataset\n",
    "\n",
    "Now it's time to split and transform the dataset.\n",
    "\n",
    "Before we split the dataset, let's remove the `sex` column because we don't need it to create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162789ba-feb2-441f-83f1-541e55a8b2ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answering these questions will help you understand the material we discussed during this session. Notice that each question could have one or more correct answers.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 1.1</strong></span></div>\n",
    "\n",
    "What will happen if we apply the SciKit-Learn transformation pipeline to the entire dataset before splitting it?\n",
    "\n",
    "1. Scaling will use the global statistics of the dataset, leaking the mean and variance of the test samples into the training process.\n",
    "2. Imputing the missing numeric values will use the global mean, leading to data leakage.\n",
    "3. The transformation pipeline expects multiple sets so it wouldn't work.\n",
    "4. We will reduce the number of lines of code we need to transform the dataset.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 1.2</strong></span></div>\n",
    "\n",
    "A hospital wants to predict which patients are prone to get a disease based on their medical history. They use weak supervision to automatically label the data using a set of heuristics. What are some of the disadvantages of weak supervision?\n",
    "\n",
    "1. Weak supervision doesn't scale to large datasets.\n",
    "2. Weak supervision doesn't adapt well to any changes that require relabeling.\n",
    "3. Weak supervision produces noisy labels.\n",
    "4. We might not be able to use weak supervision to label every data sample.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 1.3</strong></span></div>\n",
    "\n",
    "When collecting the information about the penguins, the scientists encountered a few rare species. To prevent these samples from not showing when splitting the data, they recommended using Stratified Sampling. Which of the following statements about Stratified Sampling are correct?\n",
    "\n",
    "1. Stratified Sampling assigns every sample of the population an equal chance of being selected.\n",
    "2. Stratified Sampling preserves the original distribution of different groups in the data.\n",
    "3. Stratified Sampling requires having a larger dataset compared to Random Sampling.\n",
    "4. Stratified Sampling can't be used when is not possible to divide all samples into groups.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 1.4</strong></span></div>\n",
    "\n",
    "Using more features to build a model will not necessarily lead to better predictions. Which of the following are drawbacks from adding more features?\n",
    "\n",
    "1. More features in a dataset increases the opportunity for data leakage.\n",
    "2. More features in a dataset increases the opportunity for overfitting.\n",
    "3. More features in a dataset increases the memory necessary to serve a model.\n",
    "4. More features in a dataset increases the development and maintenance time of a model. \n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 1.5</strong></span></div>\n",
    "\n",
    "A bank wants to store every transaction they handle in a set of files in the cloud. Each file will contain the transactions generated in a day. The team who will manage these files wants to optimize the storage space and downloding speed. What format should the bank use to store the transactions?\n",
    "\n",
    "1. The bank should store the data in JSON format.\n",
    "2. The bank should store the data in CSV format.\n",
    "3. The bank should store the data in Parquet format.\n",
    "4. The bank should store the data in Pandas format.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 1.1</strong></span> Familiarize yourself with the dataset we used to kick start the project. Extend the Exploratory Data Analysis in this notebook with any other analysis that you consider interesting.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 1.2</strong></span> The code in this session removes the `sex` column from the dataset before transforming the data. Modify the Scikit-Learn Pipeline to include an additional step that removes the `sex` column.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 1.3</strong></span> The code in this session uses Random Sampling to split the dataset. Modify the code to use Stratified Sampling instead.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 1.4</strong></span> You can use [Amazon SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/) to complete each step of the data preparation workflow (including data selection, cleansing, exploration, visualization, and processing at scale) from a single visual interface. For this assignment, load the Data Wrangler interface and use it to build the same transformations we implemented using the Scikit-Learn Pipeline. If you have questions, open the [Penguins Data Flow](penguins.flow) included in this repository.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 1.5</strong></span> You can use [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/data-labeling/) to label data and generate labeled synthetic data. You can configure the service to use workers from either Amazon Mechanical Turk, a vendor company, or an internal, private workforce along with machine learning to enable you to create a labeled dataset. For this assignment, familiarize yourself with the service and setup a simple \"Text Classification (Multi-label)\" labeling job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e33ac5-3a3b-4874-a45a-8a4c89060b7c",
   "metadata": {},
   "source": [
    "# Session 2 - Implementing a Production Pipeline\n",
    "\n",
    "In this session we'll build a simple [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with one step to preprocess the data.\n",
    "\n",
    "We'll use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) to execute a preprocessing script. Check the [SageMaker Pipelines Overview](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) for an introduction to the fundamental components of a SageMaker Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e1ba8f1-cea4-45cd-8217-c3c8719708fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=BUCKET)\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d06e0-b711-4e3d-b424-6fa611a51f94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Creating the Preprocessing Script\n",
    "\n",
    "We need to create a script to transform and split the dataset. This is the script that we'll run using the Processing Job.\n",
    "\n",
    "The script will save the Scikit-Learn pipeline that we use to preprocess the data and the list of target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb6ba7c0-1bd6-4fe5-8b7f-f6cbdfd3846c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/preprocessor.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import tarfile\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from pickle import dump, load\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "import shutil\n",
    "\n",
    "import csv\n",
    "import joblib\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    content_types, encoders, env, modules, transformer, worker)\n",
    "\n",
    "\n",
    "\n",
    "TARGET_COLUMN = \"species\"\n",
    "feature_columns_names = [\n",
    "    'island',\n",
    "    'culmen_length_mm',\n",
    "    'culmen_depth_mm', \n",
    "    'flipper_length_mm',\n",
    "    'body_mass_g',\n",
    "    'sex']\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    'island': \"category\",\n",
    "    'culmen_length_mm': \"float64\",\n",
    "    'culmen_depth_mm': \"float64\",\n",
    "    'flipper_length_mm': \"float64\",\n",
    "    'body_mass_g': \"float64\",\n",
    "    'sex': \"category\"}\n",
    "\n",
    "label_column_dtype = {'species': \"category\"}\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently only take csv input. Since we need to process both labelled\n",
    "    and unlabelled data we first determine whether the label column is present\n",
    "    by looking at how many columns were provided.\n",
    "    \"\"\"\n",
    "    if content_type == \"text/csv\":\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(StringIO(input_data), header=None)\n",
    "\n",
    "        if len(df.columns) == len(feature_columns_names) + 1:\n",
    "            # This is a labelled example\n",
    "            df.columns = [TARGET_COLUMN] + feature_columns_names\n",
    "            \n",
    "        elif len(df.columns) == len(feature_columns_names):\n",
    "            # This is an unlabelled example.\n",
    "            df.columns = feature_columns_names\n",
    "\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"{content_type} is not supported.!\")\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    print(\"SANTI - output_fn\", prediction)\n",
    "    \n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append(row)\n",
    "        json_output = {\"instances\": instances}\n",
    "        \n",
    "        print(\"Accept JSON:\", json_output)\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        \n",
    "        print(\"Accept CSV:\", prediction)\n",
    "        \n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(f\"{accept} accept type is not supported.\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Preprocess input data\n",
    "    \"\"\"\n",
    "    \n",
    "    features_transformer = model[\"features_transformer\"]\n",
    "    features = features_transformer.transform(input_data)\n",
    "    \n",
    "    if TARGET_COLUMN in input_data:\n",
    "        target = np.array(input_data[TARGET_COLUMN]).reshape(-1, 1)\n",
    "        \n",
    "        target_transformer = model[\"target_transformer\"]\n",
    "        target = target_transformer.transform(target)\n",
    "        \n",
    "        # return np.insert(features, 0, input_data[TARGET_COLUMN], axis=1)\n",
    "        # return np.insert(features, 0, target, axis=1)\n",
    "        return np.hstack((target, features))\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"target_transformer\": joblib.load(os.path.join(model_dir, \"target.joblib\")),\n",
    "        \"features_transformer\": joblib.load(os.path.join(model_dir, \"features.joblib\")),\n",
    "    }\n",
    "\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = [file for file in input_directory.glob(\"*.csv\")]\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        raise ValueError(f\"The are no CSV files in {str(input_directory)}/\")\n",
    "        \n",
    "    raw_data = [\n",
    "        pd.read_csv(\n",
    "            file, \n",
    "            # header=None,\n",
    "            # names=[TARGET_COLUMN] + feature_columns_names,\n",
    "            # dtype=merge_two_dicts(label_column_dtype, feature_columns_dtype)\n",
    "        ) for file in files\n",
    "    ]\n",
    "    df = pd.concat(raw_data)\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    \n",
    "    \n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])]\n",
    "    )\n",
    "    \n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler()\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder()\n",
    "    )\n",
    "    \n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"numeric\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "    \n",
    "    \n",
    "    y_train = target_transformer.fit_transform(np.array(df_train.species.values).reshape(-1, 1))\n",
    "    y_validation = target_transformer.transform(np.array(df_validation.species.values).reshape(-1, 1))\n",
    "    y_test = target_transformer.transform(np.array(df_test.species.values).reshape(-1, 1))\n",
    "    \n",
    "    \n",
    "    df_train.drop(TARGET_COLUMN, axis=1, inplace=True)\n",
    "    df_validation.drop(TARGET_COLUMN, axis=1, inplace=True)\n",
    "    df_test.drop(TARGET_COLUMN, axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    X_train = features_transformer.fit_transform(df_train)\n",
    "    X_validation = features_transformer.transform(df_validation)\n",
    "    X_test = features_transformer.transform(df_test)\n",
    "    \n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(validation_path / \"validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "    \n",
    "    \n",
    "    joblib.dump(target_transformer, \"target.joblib\")\n",
    "    joblib.dump(features_transformer, \"features.joblib\")\n",
    "    \n",
    "    \n",
    "    model_path = Path(base_directory) / \"model\"\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    with tarfile.open(f\"{str(model_path / 'model.tar.gz')}\", \"w:gz\") as tar:\n",
    "        tar.add(f\"target.joblib\")\n",
    "        tar.add(f\"features.joblib\")\n",
    "    \n",
    "    \n",
    "#     df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "#     df_train, temp = train_test_split(df, test_size=0.3)\n",
    "#     df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "    \n",
    "#     target_transformer = ColumnTransformer(\n",
    "#         transformers=[(\"species\", OrdinalEncoder(), [0])]\n",
    "#     )\n",
    "    \n",
    "#     target_transformer.fit(np.array(df_train[\"species\"].values).reshape(-1, 1))\n",
    "#     joblib.dump(target_transformer, os.path.join(model_directory, \"target.joblib\"))\n",
    "    \n",
    "#     df_train.drop(TARGET_COLUMN, axis=1, inplace=True)\n",
    "    \n",
    "#     numeric_transformer = make_pipeline(\n",
    "#         SimpleImputer(strategy=\"mean\"),\n",
    "#         StandardScaler()\n",
    "#     )\n",
    "\n",
    "#     categorical_transformer = make_pipeline(\n",
    "#         SimpleImputer(strategy=\"most_frequent\"),\n",
    "#         OneHotEncoder()\n",
    "#     )\n",
    "    \n",
    "#     features_transformer = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"numeric\", numeric_transformer, make_column_selector(dtype_exclude=\"category\")),\n",
    "#             (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     df_train = features_transformer.fit_transform(df_train)\n",
    "#     df_validation = features_transformer.transform(df_validation)\n",
    "#     df_test = features_transformer.transform(df_test)\n",
    "    \n",
    "#     joblib.dump(features_transformer, os.path.join(model_directory, \"features.joblib\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "#     parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "#     parser.add_argument(\"--train\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     fit_transformation_pipelines(\n",
    "#         input_directory=Path(args.train), \n",
    "#         model_directory=args.model_dir\n",
    "#     )\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c9464-62d4-4a4c-a6a6-d09b3b6bc90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from preprocessor import preprocess\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory: \n",
    "    data = pd.read_csv(DATA_FILEPATH)\n",
    "    \n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    data.to_csv(input_directory / \"data.csv\")\n",
    "    \n",
    "    preprocess(\n",
    "        base_directory=Path(directory)\n",
    "    )\n",
    "    \n",
    "    print(f\"output: {os.listdir(directory)}\")\n",
    "    \n",
    "    df = pd.read_csv(Path(directory) / \"train\" / \"train.csv\", header=None)\n",
    "    \n",
    "    print(df.head(10))\n",
    "#     t = joblib.load(os.path.join(directory, \"target.joblib\"))\n",
    "    \n",
    "#     x = t.transform([[\"Adelie\"]])\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e408d-686b-4af2-9e64-0d0b73c9cf69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "target = [[10], [20]]\n",
    "\n",
    "\n",
    "# np.insert(features, 0, target, axis=1)\n",
    "\n",
    "np.hstack((target, features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12ca2d6a-ece6-4346-abe7-260a66006628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/split.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/split.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "# This is the location where the SageMaker Processing job\n",
    "# will save the input dataset.\n",
    "BASE_DIRECTORY = \"/opt/ml/processing\"\n",
    "DATA_FILEPATH = Path(BASE_DIRECTORY) / \"input\" / \"data.csv\"\n",
    "\n",
    "\n",
    "def split(base_directory, data_filepath):\n",
    "    \"\"\"\n",
    "    TBD\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(data_filepath)\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "    \n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    df_train.to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    df_validation.to_csv(validation_path / \"validation.csv\", header=False, index=False)\n",
    "    df_test.to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split(BASE_DIRECTORY, DATA_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2bf51-1aca-4d02-8d73-46970a54f2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "# FRAMEWORK_VERSION = \"1.2-1\"\n",
    "\n",
    "# sklearn_preprocessor = SKLearn(\n",
    "#     entry_point=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "#     role=role,\n",
    "#     framework_version=FRAMEWORK_VERSION,\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "#     sagemaker_session=sagemaker_session,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850b6c0-f462-43a6-b11b-7e5f68836675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sklearn_preprocessor.fit({\"train\": \"s3://mlschool/penguins/data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6fe1a-b4f3-4785-aade-a39c1086da17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformer = sklearn_preprocessor.transformer(\n",
    "#     instance_count=1, \n",
    "#     instance_type=\"ml.m5.xlarge\", \n",
    "#     assemble_with=\"Line\", \n",
    "#     accept=\"text/csv\"\n",
    "# )\n",
    "\n",
    "\n",
    "# transformer.transform(\"s3://mlschool/penguins/data\", content_type=\"text/csv\")\n",
    "# print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "\n",
    "# transformer.wait()\n",
    "# preprocessed_train = transformer.output_path\n",
    "# preprocessed_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f349e92-70c4-45c6-a090-be58c7cac63e",
   "metadata": {},
   "source": [
    "## HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a07e3ce6-d487-47e4-81cd-f21793ebaca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.parameters import ParameterFloat\n",
    "from sagemaker.inputs import CreateModelInput, TransformInput\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.steps import CreateModelStep, TransformStep\n",
    "\n",
    "\n",
    "cache_config = CacheConfig(\n",
    "    enable_caching=True, \n",
    "    expire_after=\"15d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd47cee0-8932-4a55-bbdd-81a28b7203be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocessor_processor = SKLearnProcessor(\n",
    "    base_job_name=\"split-processor\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=preprocessor_processor.run(\n",
    "        code=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(source=\"s3://mlschool/penguins/data\", destination=\"/opt/ml/processing/input\"),  \n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "            ProcessingOutput(output_name=\"model\", source=\"/opt/ml/processing/model\"),\n",
    "        ]\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")\n",
    "\n",
    "# estimator1 = SKLearn(\n",
    "#     entry_point=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "#     role=role,\n",
    "#     framework_version=\"1.2-1\",\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "#     sagemaker_session=pipeline_session,\n",
    "# )\n",
    "\n",
    "\n",
    "# preprocessor_step = TrainingStep(\n",
    "#     name=\"fit-transformer\",\n",
    "#     step_args=estimator1.fit(\n",
    "#         inputs={\n",
    "#             \"train\": TrainingInput(\n",
    "#                 s3_data=split_step.properties.ProcessingOutputConfig.Outputs[\n",
    "#                     \"train\"\n",
    "#                 ].S3Output.S3Uri,\n",
    "#                 content_type=\"text/csv\"\n",
    "#             ),\n",
    "#         }\n",
    "#     ),\n",
    "#     cache_config=cache_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b4895df-8ba7-415a-b0ac-f6df4f6146ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create_model_step = ModelStep(\n",
    "#     name=\"transformer\",\n",
    "#     # display_name=\"create-model\",\n",
    "#     step_args=skmodel.create(\n",
    "#         instance_type=\"ml.m5.large\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# transformer = Transformer(\n",
    "#     model_name=create_model_step.properties.ModelName,\n",
    "#     base_transform_job_name=\"transformer\",\n",
    "\n",
    "#     instance_type=\"ml.c5.xlarge\",\n",
    "#     instance_count=1,\n",
    "#     assemble_with=\"Line\", \n",
    "#     accept=\"text/csv\",\n",
    "    \n",
    "#     # output_path=f\"s3://mlschool/cohort7/transform\",\n",
    "#     sagemaker_session=pipeline_session\n",
    "# )\n",
    "\n",
    "# transform_train_data_step = TransformStep(\n",
    "#     name=\"transform-train-data\",\n",
    "#     step_args=transformer.transform(\n",
    "#         data=split_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "#         content_type=\"text/csv\"\n",
    "#     ),\n",
    "#     cache_config=cache_config\n",
    "# )\n",
    "\n",
    "# transform_validation_data_step = TransformStep(\n",
    "#     name=\"transform-validation-data\",\n",
    "#     step_args=transformer.transform(\n",
    "#         data=split_step.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "#         content_type=\"text/csv\"\n",
    "#     ),\n",
    "#     cache_config=cache_config\n",
    "# )\n",
    "\n",
    "# transform_test_data_step = TransformStep(\n",
    "#     name=\"transform-test-data\",\n",
    "#     step_args=transformer.transform(\n",
    "#         data=split_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "#         content_type=\"text/csv\"\n",
    "#     ),\n",
    "#     cache_config=cache_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a950244-f82d-4684-989d-861052cedee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def train(model_directory, train_path, validation_path, epochs=50, batch_size=32):\n",
    "    train_files = [file for file in Path(train_path).glob(\"*.csv\")]\n",
    "    validation_files = [file for file in Path(validation_path).glob(\"*.csv\")]\n",
    "    \n",
    "    if len(train_files) == 0 or len(validation_files) == 0:\n",
    "        raise ValueError(\"The are no train or validation files\")\n",
    "        \n",
    "    train_data = [pd.read_csv(file, header=None) for file in train_files]\n",
    "    X_train = pd.concat(train_data)\n",
    "    # X_train = pd.read_csv(Path(train_path), header=None)\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    validation_data = [pd.read_csv(file, header=None) for file in validation_files]\n",
    "    X_validation = pd.concat(validation_data)\n",
    "    # X_validation = pd.read_csv(Path(validation_path), header=None)\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "    \n",
    "    model_filepath = Path(model_directory) / \"001\"\n",
    "    model.save(model_filepath)    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to \n",
    "    # the entry point as script arguments. \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "\n",
    "    train(\n",
    "        # This is the location where we need to save our model. SageMaker will\n",
    "        # create a model.tar.gz file with anything inside this directory when\n",
    "        # the training script finishes.\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "\n",
    "        # SageMaker creates one channel for each one of the inputs to the\n",
    "        # Training Step.\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n",
    "\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50f903cb-93f0-41a2-8d31-b9b897578737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sagemaker.experiments import Run\n",
    "\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tensorflow_estimator = TensorFlow(\n",
    "    base_job_name=\"penguins-training\",\n",
    "    entry_point=f\"{CODE_FOLDER}/train.py\",\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "    ],\n",
    "    \n",
    "    framework_version=\"2.6\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "train_model_step = TrainingStep(\n",
    "    name=\"train-model\",\n",
    "    step_args=tensorflow_estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocess_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            )\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "879382a6-c6cc-4256-916e-9135f02c2ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Adelie', 0.958234251), ('Adelie', 0.924666941), ('Adelie', 0.883159637)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = [[0.958234251, 0.0416116305, 0.000154100155], [0.924666941, 0.0744678229, 0.000865289418], [0.883159637, 0.11454577, 0.00229456788]]\n",
    "categories = np.array([\"Adelie\", \"Gentoo\", \"Chinstrap\"])\n",
    "predictions = np.argmax(input_data, axis=-1)\n",
    "confidence = np.max(input_data, axis=-1)\n",
    "\n",
    "result = [(categories[pred], conf) for pred, conf in zip(predictions, confidence)]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18c3d04f-d20b-49b6-928a-a2b7bb5629aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/postprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/postprocessor.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import tarfile\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from pickle import dump, load\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "import shutil\n",
    "\n",
    "import csv\n",
    "import joblib\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "try:\n",
    "    from sagemaker_containers.beta.framework import (\n",
    "        content_types,\n",
    "        encoders,\n",
    "        env,\n",
    "        modules,\n",
    "        transformer,\n",
    "        worker,\n",
    "        server,\n",
    "    )\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# {\n",
    "#     \"predictions\": [[0.821099818, 0.168315798, 0.0105843134], [0.806652844, 0.179276273, 0.0140708638], [0.845787048, 0.148024455, 0.00618851744]]\n",
    "# }\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    print(\"INPUT_FN\", input_data, content_type)\n",
    "    \n",
    "    if content_type == \"application/json\":\n",
    "        predictions = json.loads(input_data)[\"predictions\"]\n",
    "        return predictions\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"{content_type} is not supported.!\")\n",
    "\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    print(\"OUTPUT_FN\", prediction, accept)\n",
    "    \n",
    "    if accept == \"application/json\":\n",
    "        result = []\n",
    "        for p, c in prediction:\n",
    "            result.append({\n",
    "                \"prediction\": p,\n",
    "                \"confidence\": c\n",
    "            })\n",
    "            \n",
    "        print(\"result\", result)\n",
    "        return worker.Response(json.dumps(result), mimetype=accept)\n",
    "    \n",
    "    elif accept == 'text/csv':\n",
    "        \n",
    "        print(\"Accept CSV:\", prediction)\n",
    "        \n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(f\"{accept} accept type is not supported.\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    print(\"PREDICT_FN\", input_data)\n",
    "    \n",
    "    categories = model[\"target_transformer\"].named_transformers_[\"species\"].categories_[0]\n",
    "    print(\"categories\", categories)\n",
    "    \n",
    "    predictions = np.argmax(input_data, axis=-1)\n",
    "    print(\"predictions\", predictions)\n",
    "    \n",
    "    \n",
    "    confidence = np.max(input_data, axis=-1)\n",
    "    print(\"confidence\", confidence)\n",
    "    \n",
    "    result = [(categories[pred], conf) for pred, conf in zip(predictions, confidence)]\n",
    "    print(\"result\", result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"target_transformer\": joblib.load(os.path.join(model_dir, \"target.joblib\")),\n",
    "        \"features_transformer\": joblib.load(os.path.join(model_dir, \"features.joblib\")),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a193715c-7ad0-4d98-b1d9-27179a55c107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.parameters import ParameterFloat\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "\n",
    "skmodel = SKLearnModel(\n",
    "    model_data=Join(\n",
    "        on=\"/\", \n",
    "        values=[\n",
    "            preprocess_step.properties.ProcessingOutputConfig.Outputs[\"model\"].S3Output.S3Uri,\n",
    "            \"model.tar.gz\"\n",
    "        ]\n",
    "    ),\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    entry_point=\"preprocessor.py\",\n",
    "    source_dir=str(CODE_FOLDER),\n",
    ")   \n",
    "\n",
    "\n",
    "post_processing_model = SKLearnModel(\n",
    "    model_data=Join(\n",
    "        on=\"/\", \n",
    "        values=[\n",
    "            preprocess_step.properties.ProcessingOutputConfig.Outputs[\"model\"].S3Output.S3Uri,\n",
    "            \"model.tar.gz\"\n",
    "        ]\n",
    "    ),\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    entry_point=\"postprocessor.py\",\n",
    "    source_dir=str(CODE_FOLDER),\n",
    ")   \n",
    "\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    model_data=train_model_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    framework_version=\"2.6\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=\"InferencePipelineModel\", \n",
    "    role=role, \n",
    "    models=[\n",
    "        skmodel, \n",
    "        model,\n",
    "        post_processing_model\n",
    "    ],\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"PipelineModel\",\n",
    "    step_args=pipeline_model.register(\n",
    "        model_package_group_name=\"cohort7\",\n",
    "        approval_status=\"Approved\",\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=\"2.6\",\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27d268e7-d721-4ab1-9445-93035022448f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/cohort7',\n",
       " 'ResponseMetadata': {'RequestId': '556209ca-aa49-4916-a015-b0ede29df5ea',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '556209ca-aa49-4916-a015-b0ede29df5ea',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '75',\n",
       "   'date': 'Sun, 24 Sep 2023 18:19:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort7pipeline = Pipeline(\n",
    "    name=\"cohort7\",\n",
    "    steps=[\n",
    "        # split_step,\n",
    "        preprocess_step,\n",
    "        # create_model_step,\n",
    "        # transform_train_data_step,\n",
    "        # transform_validation_data_step,\n",
    "        # transform_test_data_step,\n",
    "        train_model_step,\n",
    "        register_model_step\n",
    "    ],\n",
    "    # pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "cohort7pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "efe937df-1b08-4fb5-86c1-39b1a1898834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:325223348818:pipeline/cohort7/execution/4f6zfrrfn32v', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x7f38b6fc7820>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort7pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f9ff80df-1a0a-4754-ae32-359f99274b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelPackageGroupName': 'cohort7',\n",
       " 'ModelPackageVersion': 8,\n",
       " 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:325223348818:model-package/cohort7/8',\n",
       " 'CreationTime': datetime.datetime(2023, 9, 24, 18, 19, 17, 680000, tzinfo=tzlocal()),\n",
       " 'ModelPackageStatus': 'Completed',\n",
       " 'ModelApprovalStatus': 'Approved'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "from sagemaker.workflow.parameters import ParameterBoolean\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.workflow.parameters import ParameterInteger\n",
    "\n",
    "response = sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=\"cohort7\",\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1,\n",
    ")\n",
    "\n",
    "package = response[\"ModelPackageSummaryList\"][0] if response[\"ModelPackageSummaryList\"] else None\n",
    "package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcf95444-ef5f-43e7-ae65-b42b04a81b77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: cohort7-2023-09-24-18-19-23-762\n",
      "INFO:sagemaker:Creating endpoint-config with name cohort7\n",
      "INFO:sagemaker:Creating endpoint with name cohort7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "model_package = ModelPackage(\n",
    "    model_package_arn=package[\"ModelPackageArn\"], \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "model_package.deploy(\n",
    "    endpoint_name=\"cohort7\", \n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.m5.large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b013ddcb-5bf5-438f-8b7f-ec373eb948da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"prediction\": \"Adelie\", \"confidence\": 0.958234251}, {\"prediction\": \"Adelie\", \"confidence\": 0.924666941}, {\"prediction\": \"Adelie\", \"confidence\": 0.883159637}, {\"prediction\": \"Adelie\", \"confidence\": 0.471390933}, {\"prediction\": \"Adelie\", \"confidence\": 0.966129959}, {\"prediction\": \"Adelie\", \"confidence\": 0.959097147}, {\"prediction\": \"Adelie\", \"confidence\": 0.952538788}, {\"prediction\": \"Adelie\", \"confidence\": 0.940574}, {\"prediction\": \"Adelie\", \"confidence\": 0.970270753}, {\"prediction\": \"Adelie\", \"confidence\": 0.897611558}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "# payload = \"M, 0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155\"\n",
    "\n",
    "payload = \"Torgersen, 39.5, 17.4, 186, 3800, FEMALE\" # Adelie\n",
    "\n",
    "# Adelie,Torgersen,39.5,17.4,186,3800,FEMALE\n",
    "# Adelie,Torgersen,40.3,18,195,3250,FEMALE\n",
    "# Adelie,Torgersen,NA,NA,NA,NA,NA\n",
    "# Adelie,Torgersen,36.7,19.3,193,3450,FEMALE\n",
    "# Adelie,Torgersen,39.3,20.6,190,3650,MALE\n",
    "\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=\"cohort7\", \n",
    "    sagemaker_session=sagemaker_session, \n",
    "    serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "# print(predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"}))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data = data.drop(\"species\", axis=1)\n",
    "\n",
    "pred_count = 10\n",
    "payload = data.iloc[:pred_count].to_csv(header=False, index=False)\n",
    "p = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "print(p.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "64c47a66-9266-4679-a38d-c01240ebb5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: cohort7\n",
      "INFO:sagemaker:Deleting endpoint with name: cohort7\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214fbdcb-8ee8-4104-bcd3-d7329693299e",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Preprocessing Script\n",
    "\n",
    "We can now load the script we just created and run it locally to ensure it outputs every file we need. In this case, we can call the `preprocess()` function with the local directory and the local copy of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c190c5-52b5-4ccc-8d42-847a694b8e66",
   "metadata": {},
   "source": [
    "# Session 3 - Building a Model\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) we built in the previous session with a step to train a model. We'll explore the [Training](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) and the [Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) steps. \n",
    "\n",
    "\n",
    "\n",
    "We'll introduce [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) and use them during training. For more information about this topic, check the [SageMaker Experiments' SDK documentation](https://sagemaker.readthedocs.io/en/v2.174.0/experiments/sagemaker.experiments.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "4756a3d8-d47b-4a88-a7f2-65ed7d4c1175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sagemaker.experiments import Run\n",
    "\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.parameter import IntegerParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.workflow.steps import TrainingStep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8608092-7aab-4fd2-aa99-47c2db27bdb7",
   "metadata": {},
   "source": [
    "## Step 1 - Creating the Training Script\n",
    "\n",
    "This script is responsible for training the neural network on the train data, validating the model, and saving it so we can later use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "d92b121d-dcb9-43e8-9ee3-3ececb583e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def train(model_directory, train_path, validation_path, epochs=50, batch_size=32):\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train.drop(X_train.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation.drop(X_validation.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape=(X_train.shape[1],), activation=\"relu\"),\n",
    "        Dense(8, activation=\"relu\"),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    print(f\"Validation accuracy: {accuracy_score(y_validation, predictions)}\")\n",
    "    \n",
    "    model_filepath = Path(model_directory) / \"001\"\n",
    "    model.save(model_filepath)    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to \n",
    "    # the entry point as script arguments. \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "\n",
    "    train(\n",
    "        # This is the location where we need to save our model. SageMaker will\n",
    "        # create a model.tar.gz file with anything inside this directory when\n",
    "        # the training script finishes.\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "\n",
    "        # SageMaker creates one channel for each one of the inputs to the\n",
    "        # Training Step.\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n",
    "\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0a4fa-ce70-4882-b9f5-8253df03d890",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Training Script\n",
    "\n",
    "Let's test the script we just created by running it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "14ea27ce-c453-4cb0-b309-dbecd732957e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 - 1s - loss: 1.0120 - accuracy: 0.7531 - val_loss: 1.0090 - val_accuracy: 0.7451\n",
      "Epoch 2/10\n",
      "8/8 - 0s - loss: 1.0006 - accuracy: 0.7824 - val_loss: 1.0001 - val_accuracy: 0.7255\n",
      "Epoch 3/10\n",
      "8/8 - 0s - loss: 0.9893 - accuracy: 0.7908 - val_loss: 0.9905 - val_accuracy: 0.7451\n",
      "Epoch 4/10\n",
      "8/8 - 0s - loss: 0.9772 - accuracy: 0.7950 - val_loss: 0.9812 - val_accuracy: 0.7647\n",
      "Epoch 5/10\n",
      "8/8 - 0s - loss: 0.9653 - accuracy: 0.8075 - val_loss: 0.9711 - val_accuracy: 0.7647\n",
      "Epoch 6/10\n",
      "8/8 - 0s - loss: 0.9522 - accuracy: 0.8075 - val_loss: 0.9610 - val_accuracy: 0.7843\n",
      "Epoch 7/10\n",
      "8/8 - 0s - loss: 0.9384 - accuracy: 0.8033 - val_loss: 0.9501 - val_accuracy: 0.7843\n",
      "Epoch 8/10\n",
      "8/8 - 0s - loss: 0.9238 - accuracy: 0.8033 - val_loss: 0.9385 - val_accuracy: 0.7843\n",
      "Epoch 9/10\n",
      "8/8 - 0s - loss: 0.9089 - accuracy: 0.8033 - val_loss: 0.9267 - val_accuracy: 0.7843\n",
      "Epoch 10/10\n",
      "8/8 - 0s - loss: 0.8935 - accuracy: 0.8033 - val_loss: 0.9145 - val_accuracy: 0.7647\n",
      "Validation accuracy: 0.7647058823529411\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp3mbewkwx/001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp3mbewkwx/001/assets\n"
     ]
    }
   ],
   "source": [
    "from preprocessor import preprocess\n",
    "from train import train\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    # First, we preprocess the data and create the \n",
    "    # dataset splits.\n",
    "    preprocess(\n",
    "        base_directory=directory, \n",
    "        data_filepath=DATA_FILEPATH\n",
    "    )\n",
    "\n",
    "    # Then, we train a model using the train and \n",
    "    # validation splits.\n",
    "    train(\n",
    "        model_directory=directory, \n",
    "        train_path=Path(directory) / \"train\", \n",
    "        validation_path=Path(directory) / \"validation\",\n",
    "        epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cff4c1-6510-4d99-8ae1-cb14927b87c7",
   "metadata": {},
   "source": [
    "## Step 3 - Setting up a Training Step\n",
    "\n",
    "We can now create a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) that we can add to the pipeline. This Training Step will create a SageMaker Training Job in the background, run the training script, and upload the output to S3. Check the [TrainingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TrainingStep) SageMaker's SDK documentation for more information. \n",
    "\n",
    "SageMaker uses the concept of an [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to handle end-to-end training and deployment tasks. For this example, we will use the built-in [TensorFlow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to run the training script we wrote before. The [Docker Registry Paths and Example Code](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) page contains information about the available framework versions for each region. Here, you can also check the available SageMaker [Deep Learning Container images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).\n",
    "\n",
    "Notice the list of hyperparameters defined below. SageMaker will pass these hyperparameters as arguments to the entry point of the training script.\n",
    "\n",
    "We are going to use [SageMaker Experiments](https://sagemaker.readthedocs.io/en/v2.174.0/experiments/sagemaker.experiments.html) to log information from the Training Job. For more information, check [Manage Machine Learning with Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html). The list of metric definitions will tell SageMaker which metrics to track and how to parse them from the Training Job logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "90fe82ae-6a2c-4461-bc83-bb52d8871e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "    base_job_name=\"penguins-training\",\n",
    "    entry_point=f\"{CODE_FOLDER}/train.py\",\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "    ],\n",
    "    \n",
    "    framework_version=\"2.6\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    py_version=\"py38\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d2b43-3bb5-4fe9-b3e4-cb8eb55c8a21",
   "metadata": {},
   "source": [
    "We can now create the [TrainingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TrainingStep) using the estimator we defined before.\n",
    "\n",
    "This step will receive the train and validation split from the preprocessing step as inputs. Notice how we reference both splits using preprocess step. This creates a dependency between the Training and Processing Steps.\n",
    "\n",
    "Here, we are using two input channels, `train` and `validation`. SageMaker will automatically create an environment variable corresponding to each of these channels following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAIN`: This environment variable will contain the path to the data in the `train` channel\n",
    "* `SM_CHANNEL_VALIDATION`: This environment variable will contain the path to the data in the `validation` channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "99e4850c-83d6-4f4e-a813-d5a3f4bb7486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py:297: UserWarning:\n",
      "\n",
      "Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model_step = TrainingStep(\n",
    "    name=\"train-model\",\n",
    "    step_args=estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            )\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814e258-c633-4e9a-85c5-6ed0f168b503",
   "metadata": {},
   "source": [
    "## Step 4 - Setting up a Tuning Step\n",
    "\n",
    "Let's now create a [Tuning Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning). This Tuning Step will create a SageMaker Hyperparameter Tuning Job in the background and use the training script to train different model variants and choose the best one. Check the [TuningStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "The Tuning Step requires a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) reference to configure the Hyperparameter Tuning Job.\n",
    "\n",
    "Here is the configuration that we'll use to find the best model:\n",
    "\n",
    "1. `objective_metric_name`: This is the name of the metric the tuner will use to determine the best model.\n",
    "2. `objective_type`: This is the objective of the tuner. Should it \"Minimize\" the metric or \"Maximize\" it? In this example, since we are using the validation accuracy of the model, we want the objective to be \"Maximize.\" If we were using the loss of the model, we would set the objective to \"Minimize.\"\n",
    "3. `metric_definitions`: Defines how the tuner will determine the metric's value by looking at the output logs of the training process.\n",
    "\n",
    "The tuner expects the list of the hyperparameters you want to explore. You can use subclasses of the [Parameter](https://sagemaker.readthedocs.io/en/stable/api/training/parameter.html#sagemaker.parameter.ParameterRange) class to specify different types of hyperparameters. This example explores different values for the `epochs` hyperparameter.\n",
    "\n",
    "Finally, you can control the number of jobs and how many of them will run in parallel using the following two arguments:\n",
    "\n",
    "* `max_jobs`: Defines the maximum total number of training jobs to start for the hyperparameter tuning job.\n",
    "* `max_parallel_jobs`: Defines the maximum number of parallel training jobs to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "038ff2e5-ed28-445b-bc03-4e996ec2286f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name = \"val_accuracy\",\n",
    "    objective_type=\"Maximize\",\n",
    "    hyperparameter_ranges = {\n",
    "        \"epochs\": IntegerParameter(10, 50),\n",
    "    },\n",
    "    metric_definitions = [\n",
    "        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}\n",
    "    ],\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de0743-6c9e-4895-b060-f58a6d60c50d",
   "metadata": {},
   "source": [
    "We can now create the [TuningStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep). \n",
    "\n",
    "This step will use the tuner we configured before and will receive the train and validation split from the preprocessing step as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "cf5dd9a7-8643-4fbb-8eb4-40f39011e27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tune_model_step = TuningStep(\n",
    "    name = \"tune-model\",\n",
    "    step_args=tuner.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c9362-ef17-4d68-b8c8-cefe21326ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5 - Switching Between Training and Tuning\n",
    "\n",
    "We could use a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) or use a [Tuning Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-tuning) to create the model.\n",
    "\n",
    "In this notebook, we will alternate between both methods and use the `USE_TUNING_STEP` flag to indicate which approach we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "c3df0f7a-e43a-4a7b-9809-0a639fa178d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_TUNING_STEP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babe38c-1682-42d2-8442-101d17aa89b5",
   "metadata": {},
   "source": [
    "## Step 6 - Setting up the Pipeline\n",
    "\n",
    "We can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "9799ab39-fcae-41f4-a68b-85ab71b3ba9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:325223348818:pipeline/penguins-session3',\n",
       " 'ResponseMetadata': {'RequestId': 'ac20e8b9-ff2e-4e8c-b3bc-e8e7c5a9cafa',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ac20e8b9-ff2e-4e8c-b3bc-e8e7c5a9cafa',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Sat, 23 Sep 2023 19:07:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session3_pipeline = Pipeline(\n",
    "    name=\"penguins-session3\",\n",
    "    parameters=[\n",
    "        dataset_location\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "session3_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044516e-f56c-4c91-8d94-6ef109eb7325",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Answering these questions will help you understand the material we discussed during this session. Notice that each question could have one or more correct answers.\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 3.1</strong></span></div>\n",
    "\n",
    "Why do we use the `SparseCategoricalCrossentropy` loss function to train our model instead of the `CategoricalCrossentropy` function?\n",
    "\n",
    "1. Because our target column are integer values.\n",
    "2. Because our target column is one-hot encoded.\n",
    "3. Because our target column are categorical values.\n",
    "4. Because there are a lot of sparse values in our dataset.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 3.2</strong></span></div>\n",
    "\n",
    "When a Training Job finishes, SageMaker automatically uploads the model to S3. Which of the following statements about this process is correct?\n",
    "\n",
    "1. SageMaker automatically creates a `model.tar.gz` file with the entire content of the `/opt/ml/model` directory.\n",
    "2. SageMaker automatically creates a `model.tar.gz` file with any files inside the `/opt/ml/model` directory as long as those files belong to the model we trained.\n",
    "3. SageMaker automatically creates a `model.tar.gz` file with any new files created inside the container by the training script.\n",
    "4. SageMaker automatically creates a `model.tar.gz` file with the content of the output folder configured in the training script.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 3.3</strong></span></div>\n",
    "\n",
    "Our pipeline uses \"file mode\" to provide the Training Job access to the dataset. When using file mode, SageMaker downloads the training data from S3 to a local directory in the training container. Imagine we have a large dataset and don't want to wait for SageMaker to download every time we want to train a model. How can we solve this problem?\n",
    "\n",
    "1. We can train our model with a smaller portion of the dataset.\n",
    "2. We can increase the number of instances and train many models in parallel.\n",
    "3. We can use \"fast file mode\" to get file system access to S3.\n",
    "4. We can use \"pipe mode\" to stream data directly from S3 into the training container.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 3.4</strong></span></div>\n",
    "\n",
    "Which of the following statements are true about the usage of `max_jobs` and `max_parallel_jobs` when running a Hyperparameter Tuning Job?\n",
    "\n",
    "1. `max_jobs` represents the maximum total number of Training Jobs that the Hyperparameter Tuning Job will start. \n",
    "2. `max_parallel_jobs` represents the maximum total number of Training Jobs that will run in parallel at any given time during a Hyperparameter Tuning Job.\n",
    "3. `max_parallel_jobs` can never be larger than `max_jobs`.\n",
    "4. `max_jobs` can never be larger than `max_parallel_jobs`.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 3.5</strong></span></div>\n",
    "\n",
    "Which of the following statements are true about tuning hyperparameters as part of a pipeline?\n",
    "\n",
    "1. Hyperparameter Tuning Jobs that don't use Amazon algorithms require a regular expression to extract the objective metric from the logs.\n",
    "2. When using a Tuning Step as part of a pipeline, SageMaker will create as many Hyperparameter Tuning Jobs as specified by the `HyperparameterTuner.max_jobs` attribute.\n",
    "3. Hyperparameter Tuning Jobs support Bayesian, Grid Search, and Random Search strategies.\n",
    "4. Using a Tuning Step is more expensive than using a Training Step.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 3.1</strong></span> The training script is using a hard-coded learning rate value to train the model. Modify the code to accept the learning rate as a parameter that we can control from outside the script.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 3.2</strong></span> We currently define the number of epochs to train the model as a constant that we pass to the Estimator using the list of hyperparameters. Replace this constant with a new Pipeline Parameter named `training_epochs`. You'll need to specify this new parameter when creating the Pipeline.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 3.3</strong></span> The goal of the current tuning process is to find the model with the highest validation accuracy. Modify the code to focus on the model with the lowest training loss.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 3.4</strong></span> Compare the validation accuracy of the last few runs of your experiments. To analyze experiment runs, select the experiment in the SageMaker Studio Experiments UI, select the runs that you want to compare, and click on the Analyze button. Check [Create an Amazon SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html) documentation for more information.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 3.5</strong></span> Modify the pipeline you created for the \"Pipeline of Digits\" project and add a Training Step. This Training Step should receive the train and validation splits from the Preprocessing step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d40fe8-ba74-4c12-9555-d8ea33d1c8b4",
   "metadata": {},
   "source": [
    "# Session 4  - Evaluating and Registering The Model\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with a step to evaluate and register the model if it reaches a predefined accuracy threshold. We'll use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor) running TensorFlow to execute an evaluation script. We'll use a [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) to determine whether the model's accuracy is above a threshold and a [Model Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-model) to register the model. To learn more about the Model Registry, check [Register and Deploy Models with Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "34fc262f-a1cf-4f94-9c60-e7c8e83cfdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.parameters import ParameterFloat\n",
    "\n",
    "\n",
    "MODEL_PACKAGE_GROUP = \"penguins\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa9691-f49f-48af-b272-3d4d17563b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - Evaluating the Model\n",
    "\n",
    "This script is responsible for loading the model we created and evaluating it on the test set. Before finishing, this script will generate an evaluation report of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "3ee3ab26-afa5-4ceb-9f7a-005d5fdea646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/evaluation.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/processing/model/\"\n",
    "TEST_PATH = \"/opt/ml/processing/test/\"\n",
    "OUTPUT_PATH = \"/opt/ml/processing/evaluation/\"\n",
    "\n",
    "\n",
    "def evaluate(model_path, test_path, output_path):\n",
    "    # The first step is to extract the model package so we can load \n",
    "    # it in memory.\n",
    "    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=Path(model_path))\n",
    "        \n",
    "    model = keras.models.load_model(Path(model_path) / \"001\")\n",
    "    \n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test.drop(X_test.columns[-1], axis=1, inplace=True)\n",
    "    \n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "    # Let's create an evaluation report using the model accuracy.\n",
    "    evaluation_report = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": accuracy\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n",
    "        f.write(json.dumps(evaluation_report))\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\n",
    "        model_path=MODEL_PATH, \n",
    "        test_path=TEST_PATH,\n",
    "        output_path=OUTPUT_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc79a0-adfd-4ce9-8580-5cd228c3c2d9",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Evaluation Script\n",
    "\n",
    "Let's test the script we just created by running it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2540d8-278a-4953-bc54-0469d154427d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from preprocessor import preprocess\n",
    "from train import train\n",
    "from evaluation import evaluate\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    preprocess(\n",
    "        base_directory=directory, \n",
    "        data_filepath=DATA_FILEPATH\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        model_directory=directory, \n",
    "        train_path=Path(directory) / \"train\", \n",
    "        validation_path=Path(directory) / \"validation\",\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    # After training a model, we need to prepare a package just like\n",
    "    # SageMaker would. This package is what the evaluation script is\n",
    "    # expecting as an input.\n",
    "    with tarfile.open(Path(directory) / \"model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(Path(directory) / \"001\", arcname=\"001\")\n",
    "        \n",
    "    \n",
    "    # We can now call the evaluation script.\n",
    "    evaluate(\n",
    "        model_path=directory, \n",
    "        test_path=Path(directory) / \"test\",\n",
    "        output_path=Path(directory) / \"evaluation\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1109a-6c26-4464-8338-94960729d212",
   "metadata": {},
   "source": [
    "## Step 3 - Setting up a Processing Step\n",
    "\n",
    "To run the evaluation script, we can use a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing). In this case, we'll use the [TensorFlowProcessor](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks-tensorflow.html) because we need access to TensorFlow. \n",
    "\n",
    "The inputs of this Processing Step will be the model we created and the test set. The output will be the evaluation report file.\n",
    "\n",
    "We can use the `USE_TUNING_STEP` flag to determine whether we created the model using a Training Step or a Tuning Step. In case we are using the Tuning Step, we can use the [TuningStep.get_top_model_s3_uri()](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.TuningStep.get_top_model_s3_uri) function to get the model artifacts from the top performing training job of the Hyperparameter Tuning Job.\n",
    "\n",
    "The [ProcessingStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) lets us specify a list of [PropertyFile](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.properties.PropertyFile) instances from the output of the job. We can use this to map the evaluation report generated in the evaluation script. Check [How to Build and Manage Property Files](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48139a07-5c8e-4bc6-b666-bf9531f7f520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensorflow_processor = TensorFlowProcessor(\n",
    "    base_job_name=\"penguins-evaluation-processor\",\n",
    "    framework_version=\"2.6\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "# We want to map the evaluation report that we generate inside\n",
    "# the evaluation script so we can later reference it.\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"evaluation-report\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Notice how this step uses the model generated by the tuning or training\n",
    "# step, and the test set generated by the preprocessing step.\n",
    "evaluate_model_step = ProcessingStep(\n",
    "    name=\"evaluate-model\",\n",
    "    step_args=tensorflow_processor.run(\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=(\n",
    "                    tune_model_step.get_top_model_s3_uri(top_k=0, s3_bucket=pipeline_session.default_bucket()) \n",
    "                    if USE_TUNING_STEP \n",
    "                    else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "                ),\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "        ],\n",
    "        code=f\"{CODE_FOLDER}/evaluation.py\"\n",
    "    ),\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a48e6-507d-42fb-8abc-ebde997e3d43",
   "metadata": {},
   "source": [
    "## Step 4 - Configuring the Model Metrics\n",
    "\n",
    "When we register a model, we can specify a set of [ModelMetrics](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html#sagemaker.model_metrics.ModelMetrics). We can use the evaluation report we generated during the Evaluation step to populate these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982146f-0c0f-4938-b5d0-06db45a58531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(on=\"/\", values=[\n",
    "            evaluate_model_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'],\n",
    "            \"evaluation.json\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fac3a-5ecc-441f-84c3-717c4c7ba290",
   "metadata": {},
   "source": [
    "## Step 5 - Registering the Model\n",
    "\n",
    "We can now create a [Model Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-model) to register the model. Check the [ModelStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.model_step.ModelStep) SageMaker's SDK documentation for more information. We want to create a new version of the model and register it in the Model Registry. Check [Register a Model Version](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-version.html) for more information about model registration.\n",
    "\n",
    "The model we trained uses TensorFlow, so we can use the built-in [TensorFlowModel](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model) class to create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a48cef-fb78-412b-a5c6-977eafe98e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TensorFlowModel(\n",
    "    model_data=(\n",
    "        tune_model_step.get_top_model_s3_uri(top_k=0, s3_bucket=pipeline_session.default_bucket())\n",
    "        if USE_TUNING_STEP\n",
    "        else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "    ),\n",
    "    framework_version=\"2.6\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"register-model\",\n",
    "    step_args=model.register(\n",
    "        model_package_group_name=MODEL_PACKAGE_GROUP,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"Approved\",\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=\"2.6\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c110f7-fe72-4db8-9d06-cfb9a0f2bfbd",
   "metadata": {},
   "source": [
    "## Step 6 - Setting up a Condition Step\n",
    "\n",
    "We only want to register a new model if its accuracy exceeds a predefined threshold. We can use a [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) together with the evaluation report we generated to accomplish this. Check the [ConditionStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#conditionstep) SageMaker's SDK documentation for more information. In this example, we will use a [ConditionGreaterThanOrEqualTo](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.conditions.ConditionGreaterThanOrEqualTo) condition to compare the model's accuracy with the threshold. Look at the [Conditions](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html#conditions) section in the documentation for more information about the types of supported conditions.\n",
    "\n",
    "If the model's accuracy is not greater than or equal our threshold, we will send the pipeline to a [Fail Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-fail) with the appropriate error message. Check the [FailStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.fail_step.FailStep) SageMaker's SDK documentation for more information.\n",
    "\n",
    "We are going to use a new [Pipeline Parameter](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html) in our pipeline to specify the minimum accuracy that the model should reach for it to be registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2a2b1-6711-4266-95d8-d2aebd52e199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_threshold = ParameterFloat(\n",
    "    name=\"accuracy_threshold\", \n",
    "    default_value=0.70\n",
    ")\n",
    "\n",
    "fail_step = FailStep(\n",
    "    name=\"fail\",\n",
    "    error_message=Join(\n",
    "        on=\" \", \n",
    "        values=[\n",
    "            \"Execution failed because the model's accuracy was lower than\", \n",
    "            accuracy_threshold\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "condition_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluate_model_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\"\n",
    "    ),\n",
    "    right=accuracy_threshold\n",
    ")\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[register_model_step],\n",
    "    else_steps=[fail_step], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309b8fa-f03e-4959-853f-dc2416f82bdd",
   "metadata": {},
   "source": [
    "## Step 7 - Setting up the Pipeline\n",
    "\n",
    "We can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bcd33-b499-4e2b-953e-94d1ed96c10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session4_pipeline = Pipeline(\n",
    "    name=\"penguins-session4\",\n",
    "    parameters=[\n",
    "        dataset_location,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step, \n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "session4_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418693c-ccd5-42b6-8ec4-04bb70fe213c",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Answering these questions will help you understand the material we discussed during this session. Notice that each question could have one or more correct answers.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 4.1</strong></span></div>\n",
    "\n",
    "When registering a model in the Model Registry, we can specify a set of metrics that will be stored with the model. Which of the following are some of the metrics supported by SageMaker?\n",
    "\n",
    "1. Metrics that measure the bias in a model.\n",
    "2. Metrics that help explain a model.\n",
    "3. Metrics that measure the quality of the input data for a model.\n",
    "4. Metrics that measure the quality of a model.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 4.2</strong></span></div>\n",
    "\n",
    "We use the `Join` function to build the error message for the Fail Step. Imagine we want to build a Amazon S3 URI. What would be the output of executing `Join(on='/', values=['s3:/', \"mlschool\", \"/\", \"12345\"])`?\n",
    "\n",
    "1. The output will be `s3://mlschool/12345`\n",
    "2. The output will be `s3://mlschool/12345/`\n",
    "3. The output will be `s3://mlschool//12345`\n",
    "4. The output will be `s3:/mlschool//12345`\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 4.3</strong></span></div>\n",
    "\n",
    "Which of the following statements are correct about the Condition Step in SageMaker:\n",
    "\n",
    "1. `ConditionComparison` is a supported condition type.\n",
    "2. `ConditionIn` is a supported condition type.\n",
    "3. When using multiple conditions together, the step will succeed if at least one of the conditions returns True.\n",
    "4. When using multiple conditions together, every one of them has to return True for the step to succeed.\n",
    "\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 4.4</strong></span></div>\n",
    "\n",
    "Imagine we use a Tuning Step to run 100 Training Jobs. The best model should have the higest validation accuracy, but we made a mistake and used \"Minimize\" as the objective type instead of \"Maximize.\" The consequence is that the index of our best model is 100 instead of 0. How can we retrieve the best model from the Tuning Step?\n",
    "\n",
    "1. We can use `TuningStep.get_top_model_s3_uri(top_k=0)` to retrieve the best model.\n",
    "2. We can use `TuningStep.get_top_model_s3_uri(top_k=100)` to retrieve the best model.\n",
    "3. We can use `TuningStep.get_bottom_model_s3_uri(top_k=0)` to retrieve the best model.\n",
    "4. In this example, we can't retrieve the best model.\n",
    "\n",
    "<div style=\"margin: 30px 0 10px 0;\"><span style=\"font-size: 1.1em; padding:4px; background-color: #b8bf9f; color: #000;\"><strong>Question 4.5</strong></span></div>\n",
    "\n",
    "\n",
    "If the accuracy of the model is above the threshold, our pipeline registers it in the Model Registry. Which of the following functions are related to the Model Registry?\n",
    "\n",
    "1. Model versioning: We can use the Model Registry to track different versions of a model, especially as it gets updated or refined over time.\n",
    "2. Model deployment: We can initiate the deployment of a model right from the Model Registry.\n",
    "3. Model metrics: The Model Registry provides insights about a particular model through the registration of metrics.\n",
    "4. Model features: The Model Registry lists every feature that was used to build the model.\n",
    "\n",
    "## Assignments\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 4.1</strong></span> Instead of running the entire pipeline from start to finish, sometimes you may only need to iterate over particular steps. SageMaker Pipelines supports [Selective Execution for Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-selective-ex.html). In this assignment you will use Selective Execution to only run the Training Step using a different number of epochs. To configure the new value, use the Pipeline Property `training_epochs` that you added during the assignments from the previous session. [Unlocking efficiency: Harnessing the power of Selective Execution in Amazon SageMaker Pipelines](https://aws.amazon.com/blogs/machine-learning/unlocking-efficiency-harnessing-the-power-of-selective-execution-in-amazon-sagemaker-pipelines/) is a great article that explains this feature.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 4.2</strong></span> The current pipeline uses a Training Step or a Tuning Step to build a model. Modify the pipeline to use both steps at the same time and register only the best model coming out from them.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 4.3</strong></span> The evaluation script loads the model from a hard-coded \"001\" directory. Instead of doing this, let's supply this directory name as an argument to the Processing Job. To do this, check the `arguments` parameter of the [Processor.run](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.Processor.run) function.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 4.4</strong></span> The evaluation script produces an evaluation report containing the accuracy of the model. Extend the evaluation report by adding the precision of the model. We want this new metric to show under the Model Quality section in the Model Registry.\n",
    "\n",
    "* <span style=\"padding:4px; background-color: #f2a68a; color: #000;\"><strong>Assignment 4.5</strong></span> Modify the SageMaker Pipeline you created for the \"Pipeline of Digits\" project and add the necessary steps to evaluate and register the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cf77e-7fc7-406e-a2e2-40c553f459f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 5 - Deploying The Model\n",
    "\n",
    "This session extends the [SageMaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) with a step to deploy the model to an endpoint. We'll use a [Lambda Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda) to create an endpoint and deploy the model. To control the endpoint's inputs and outputs, we'll modify the model's assets to include code that customizes the processing of a request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d585140-7940-4543-9954-b74352e8ff3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "from sagemaker.workflow.parameters import ParameterBoolean\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.workflow.parameters import ParameterInteger\n",
    "\n",
    "ENDPOINT = \"penguins-endpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93727425-fac6-44ec-91ed-130a50fdd18a",
   "metadata": {},
   "source": [
    "## Step 1 - Deploy Latest Model From Registry\n",
    "\n",
    "Let's deploy the latest model from the Model Registry using SageMaker's SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a22326-a821-4f03-80f0-f4155279a1b8",
   "metadata": {},
   "source": [
    "To deploy a model from the Model Registry, we need to find its model package ARN. Let's query the list of approved models and get the latest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87437a26-e9ea-4866-9dc3-630444c0fb46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sagemaker_client.list_model_packages(\n",
    "    ModelPackageGroupName=MODEL_PACKAGE_GROUP,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\",\n",
    "    MaxResults=1,\n",
    ")\n",
    "\n",
    "package = response[\"ModelPackageSummaryList\"][0] if response[\"ModelPackageSummaryList\"] else None\n",
    "package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3119b48-2ddf-40b5-9ac0-680073a53d06",
   "metadata": {},
   "source": [
    "Using the ARN of the model package from the Model Registry, we can deploy the model by creating a [ModelPackage](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.ModelPackage) instance and calling its `deploy()` function. The model information lives in the Model Registry, so we don't need to specify anything else.\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#0066cc;\">Uncomment the <code style=\"background-color:#0066cc;\">%%script</code> cell magic line to execute this cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8852d5-818a-406c-944d-30bf6de90288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "model_package = ModelPackage(\n",
    "    model_package_arn=package[\"ModelPackageArn\"], \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "model_package.deploy(\n",
    "    endpoint_name=ENDPOINT, \n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.m5.large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcfffa-0ba6-4ad8-8b4f-1ea19b35a22f",
   "metadata": {},
   "source": [
    "Let's test the endpoint to make sure it works. Notice the payload we need to provide the model is in CSV format. The model expects data that's already transformed. We can't provide the original data from our dataset because the model will not work with it.\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#0066cc;\">Uncomment the <code style=\"background-color:#0066cc;\">%%script</code> cell magic line to execute this cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817a25e-8224-4911-830b-d659e7458b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# We'll send this payload to the endpoint. Notice how each line contains\n",
    "# the information of a penguin. The endpoint will return the predictions\n",
    "# for each of these lines.\n",
    "payload = \"\"\"\n",
    "0.6569590202313976, -1.0813829646495108, 1.2097102831892812, 0.9226343641317372, 1.0, 0.0, 0.0\n",
    "-0.7751048801481084, 0.8822689351285553,  -1.2168066120762704, 0.9226343641317372, 0.0, 1.0, 0.0\n",
    "-0.837387834894918, 0.3386660813829646, -0.26237731892812, -1.92351941317372, 0.0, 0.0, 1.0\n",
    "\"\"\"\n",
    "\n",
    "# We can now send the request to the endpoint and process the response.\n",
    "predictor = Predictor(endpoint_name=ENDPOINT)\n",
    "response = predictor.predict(payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "response = json.loads(response.decode(\"utf-8\"))\n",
    "\n",
    "print(json.dumps(response, indent=2))\n",
    "print(f\"\\nSpecies: {np.argmax(response['predictions'], axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5d383-fcd7-454c-bbd6-ce4ce7b2104a",
   "metadata": {},
   "source": [
    "We can now delete the endpoint using the predictor.\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#0066cc;\">Uncomment the <code style=\"background-color:#0066cc;\">%%script</code> cell magic line to execute this cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32c3a4-312e-473c-a217-33606f77d1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc27c6-31d4-454d-ae5b-1aeba25f0ac0",
   "metadata": {},
   "source": [
    "## Step 2 - Preparing the Inference Code\n",
    "\n",
    "Deploying the model we trained directly to an endpoint doesn't lets us control the data that goes in and comes out of the endpoint. Fortunately, SageMaker allows us to include an `inference.py` file with the model assets from where we can control how the endpoint works. You can see more information about how this works by checking the [SageMaker TensorFlow Serving Container](https://github.com/aws/sagemaker-tensorflow-serving-container) documentation.\n",
    "\n",
    "We want our endpoint to handle unprocessed data in JSON format and return the penguin's species. Here is an example of the payload we want the endpoint to support:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"island\": \"Biscoe\",\n",
    "    \"culmen_length_mm\": 48.6,\n",
    "    \"culmen_depth_mm\": 16.0,\n",
    "    \"flipper_length_mm\": 230.0,\n",
    "    \"body_mass_g\": 5800.0,\n",
    "}\n",
    "```\n",
    "\n",
    "And here is an example of the output we'd like to get from the endpoint:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prediction\": \"Adelie\", \n",
    "    \"confidence\": 0.802672\n",
    "}\n",
    "```\n",
    "\n",
    "Let's start by setting up a local folder where we will create the `inference.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9cec59-c812-4ca9-9f71-d6725de03c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT_CODE_FOLDER = CODE_FOLDER / \"endpoint\"\n",
    "Path(ENDPOINT_CODE_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "sys.path.append(f\"./{ENDPOINT_CODE_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34c1c1-66af-4cf2-b103-8612bd60ce0e",
   "metadata": {},
   "source": [
    "We will include the inference code as part of the model assets to control the inference process on the SageMaker endpoint. SageMaker will automatically call the `handler()` function for every request to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7ad9e-598b-4e28-9a20-a93ad2bb5f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {ENDPOINT_CODE_FOLDER}/inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pickle import load\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def handler(\n",
    "    data, \n",
    "    context, \n",
    "    pipeline_file=Path(\"/tmp\") / \"pipeline.pkl\", \n",
    "    classes_file=Path(\"/tmp\") / \"classes.csv\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the entrypoint that will be called by SageMaker when the endpoint\n",
    "    receives a request. You can see more information at \n",
    "    https://github.com/aws/sagemaker-tensorflow-serving-container.\n",
    "    \"\"\"\n",
    "    print(\"Handling endpoint request\")\n",
    "    \n",
    "    processed_input, groundtruth = _process_input(data, context, pipeline_file)\n",
    "    output = _predict(processed_input, context) if processed_input else None\n",
    "    return _process_output(output, groundtruth, context, classes_file)\n",
    "\n",
    "\n",
    "def _process_input(data, context, pipeline_file):\n",
    "    print(\"Processing input data...\")\n",
    "    \n",
    "    if context is None:\n",
    "        # The context will be None when we are testing the code\n",
    "        # directly from a notebook. In that case, we can use the\n",
    "        # data directly.\n",
    "        endpoint_input = data\n",
    "    elif context.request_content_type in (\"application/json\", \"application/octet-stream\"):\n",
    "        # When the endpoint is running, we will receive a context\n",
    "        # object. We need to parse the input and turn it into \n",
    "        # JSON in that case.\n",
    "        endpoint_input = json.loads(data.read().decode(\"utf-8\"))\n",
    "\n",
    "        if endpoint_input is None:\n",
    "            raise ValueError(\"There was an error parsing the input request.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {context.request_content_type or 'unknown'}\")\n",
    "        \n",
    "    groundtruth = endpoint_input.pop(\"species\", None)\n",
    "        \n",
    "    df = pd.json_normalize(endpoint_input)\n",
    "    \n",
    "    try:\n",
    "        pipeline = _get_pipeline(pipeline_file)\n",
    "        result = pipeline.transform(df)\n",
    "    except Exception as e:\n",
    "        print(f\"There was an error transforming the input data. {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    return result[0].tolist(), groundtruth\n",
    "\n",
    "\n",
    "def _predict(instance, context):\n",
    "    print(\"Sending input data to model to make a prediction...\")\n",
    "    \n",
    "    model_input = json.dumps({\"instances\": [instance]})\n",
    "    \n",
    "    if context is None:\n",
    "        # The context will be None when we are testing the code\n",
    "        # directly from a notebook. In that case, we want to return\n",
    "        # a fake prediction back.\n",
    "        result = {\n",
    "            \"predictions\": [\n",
    "                [0.2, 0.5, 0.3]\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        # When the endpoint is running, we will receive a context\n",
    "        # object. In that case we need to send the instance to the\n",
    "        # model to get a prediction back.\n",
    "        response = requests.post(context.rest_uri, data=model_input)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(response.content.decode('utf-8'))\n",
    "            \n",
    "        result = json.loads(response.content)\n",
    "    \n",
    "    print(f\"Response: {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def _process_output(output, groundtruth, context, classes_file):\n",
    "    print(\"Processing prediction received from the model...\")\n",
    "    \n",
    "    if output:\n",
    "        prediction = np.argmax(output[\"predictions\"][0])\n",
    "        confidence = output[\"predictions\"][0][prediction]\n",
    "\n",
    "        result = {\n",
    "            \"prediction\": _get_class(prediction, classes_file),\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "\n",
    "        if groundtruth:\n",
    "            result[\"groundtruth\"] = groundtruth\n",
    "            \n",
    "    else:\n",
    "        result = {\n",
    "            \"prediction\": None\n",
    "        }\n",
    "\n",
    "    print(result)\n",
    "    \n",
    "    response_content_type = \"application/json\" if context is None else context.accept_header\n",
    "    return json.dumps(result), response_content_type\n",
    "\n",
    "\n",
    "def _get_pipeline(pipeline_file):\n",
    "    \"\"\"\n",
    "    This function returns the Scikit-Learn pipeline we used to transform the\n",
    "    dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    _download(pipeline_file, os.environ.get(\"PIPELINE_S3_LOCATION\", None))\n",
    "    return load(open(pipeline_file, 'rb'))\n",
    "\n",
    "\n",
    "def _get_class(prediction, classes_file):\n",
    "    \"\"\"\n",
    "    This function returns the class name of a given prediction. \n",
    "    \"\"\"\n",
    "    \n",
    "    _download(classes_file, os.environ.get(\"CLASSES_S3_LOCATION\", None))\n",
    "    \n",
    "    with open(classes_file) as f:\n",
    "        file = f.readlines()\n",
    "        \n",
    "    classes = list(map(lambda x: x.replace(\"'\", \"\"), file[0].split(',')))\n",
    "    return classes[prediction]\n",
    "\n",
    "\n",
    "def _download(file, s3_location):\n",
    "    \"\"\"\n",
    "    This function downloads a file from S3 if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    if file.exists():\n",
    "        return\n",
    "        \n",
    "    s3_parts = s3_location.split('/', 3)\n",
    "    bucket = s3_parts[2]\n",
    "    key = s3_parts[3]\n",
    "    \n",
    "    s3.Bucket(bucket).download_file(key, str(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7e63-39a8-4859-af1e-3e7b1573f9a2",
   "metadata": {},
   "source": [
    "## Step 3 - Testing the Inference Code\n",
    "\n",
    "Let's test the inference code locally to ensure it works before deploying it. The `handler()` function is the entry point that will be called by SageMaker whenever the endpoint receives a request.\n",
    "\n",
    "When testing the inference code, we want to set the `context` to `None` so the function recognizes we are calling it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aca49b-2080-4068-80e6-8b3f10e54177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from preprocessor import preprocess\n",
    "from inference import handler, _get_class\n",
    "\n",
    "\n",
    "samples = [\n",
    "    {\n",
    "        \"island\": \"Biscoe\",\n",
    "        \"culmen_length_mm\": 48.6,\n",
    "        \"culmen_depth_mm\": 16.0,\n",
    "        \"flipper_length_mm\": 230.0,\n",
    "        \"body_mass_g\": 5800.0,\n",
    "        \"species\": \"Biscoe\"\n",
    "    },\n",
    "    {\n",
    "        \"island\": \"Unknown\",\n",
    "        \"culmen_length_mm\": 48.6,\n",
    "        \"culmen_depth_mm\": 16.0,\n",
    "        \"flipper_length_mm\": 230.0,\n",
    "        \"body_mass_g\": 5800.0,\n",
    "    }\n",
    "]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    \n",
    "    # We want to call the `preprocess()` function to generate the\n",
    "    # `pipeline.pkl` and `classes.csv` files that we need to use\n",
    "    # as part of the endpoint inference script.\n",
    "    preprocess(\n",
    "        base_directory=directory, \n",
    "        data_filepath=DATA_FILEPATH\n",
    "    )\n",
    "    \n",
    "    for sample in samples:\n",
    "        handler(\n",
    "            data=sample, \n",
    "            context=None,\n",
    "            pipeline_file=Path(directory) / \"pipeline\" / \"pipeline.pkl\",\n",
    "            classes_file=Path(directory) / \"classes\" / \"classes.csv\"\n",
    "        )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63f158-06f3-438b-b489-bb0496f2eddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4 - Registering the Model\n",
    "\n",
    "We can now register a new [TensorFlowModel](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model). We must also ensure SageMaker repackages the model assets to include the `inference.py` file.\n",
    "\n",
    "SageMaker triggers a repack whenever we specify the `source_dir` attribute. We want that attribute to point to the local folder containing the `inference.py` file. SageMaker will automatically modify the original `model.tar.gz` package to include a `/code` folder containing the file. Since we need access to Scikit-Learn in our script, we can include a `requirements.txt` file in the same `/code` folder, and SageMaker will install everything in it. To repack the model assets, SageMaker will automatically include a new step in the pipeline right before registering the model.\n",
    "\n",
    "Here is what the new `model.tar.gz` package will look like:\n",
    "\n",
    "```\n",
    "model/\n",
    "    |--[model_version_number]\n",
    "        |--assets/\n",
    "        |--variables/\n",
    "        |--saved_model.pb\n",
    "code/\n",
    "    |--inference.py\n",
    "    |--requirements.txt\n",
    "```\n",
    "\n",
    "Let's use a [ModelStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.model_step.ModelStep) to register the model. Notice the following:\n",
    "\n",
    "* `model_data`: We use the model assets we generated during the Training or Tuning Step. We determined which assets to use back in Session 4 and stored them in the `model_data` variable.\n",
    "* `source_dir`: This points to the local folder containing the `inference.py` file. SageMaker will trigger a repack to include the `/code` folder in the model assets.\n",
    "* `env`: Our custom inference code expects an environment variable `S3_LOCATION` to point to the location of the Scikit-Learn pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e2c6d-977e-43ec-98d4-ec66781af582",
   "metadata": {},
   "source": [
    "SageMaker's default TensorFlow inference container doesn't come with Scikit-Learn installed, so we need to provide a `requirements.txt` file with the libraries we want SageMaker to install in our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40d098-d553-4d56-b2eb-f80cec420ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {ENDPOINT_CODE_FOLDER}/requirements.txt\n",
    "\n",
    "numpy==1.19.5\n",
    "pandas==1.2.5\n",
    "scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07513884-c7bd-4710-9730-f8ca7fe904a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TensorFlowModel(\n",
    "    name=\"penguins\",\n",
    "    model_data=(\n",
    "        tune_model_step.get_top_model_s3_uri(top_k=0, s3_bucket=pipeline_session.default_bucket())\n",
    "        if USE_TUNING_STEP\n",
    "        else train_model_step.properties.ModelArtifacts.S3ModelArtifacts\n",
    "    ),\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=str(ENDPOINT_CODE_FOLDER),\n",
    "    env={\n",
    "        \"PIPELINE_S3_LOCATION\": Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\"pipeline\"].S3Output.S3Uri,\n",
    "                \"pipeline.pkl\",\n",
    "            ]\n",
    "        ),\n",
    "        \"CLASSES_S3_LOCATION\": Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                preprocess_data_step.properties.ProcessingOutputConfig.Outputs[\"classes\"].S3Output.S3Uri,\n",
    "                \"classes.csv\",\n",
    "            ]\n",
    "        )\n",
    "    },\n",
    "    framework_version=\"2.6\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"register\",\n",
    "    display_name=\"register-model\",\n",
    "    step_args=model.register(\n",
    "        model_package_group_name=MODEL_PACKAGE_GROUP,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"Approved\",\n",
    "        content_types=[\"application/json\"],\n",
    "        response_types=[\"application/json\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"TENSORFLOW\",\n",
    "        framework_version=\"2.6\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b60d5-6be3-428f-9979-4fe9eebfc5eb",
   "metadata": {},
   "source": [
    "## Step 5 - Creating the Lambda Function\n",
    "\n",
    "Let's use a [Lambda Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-lambda) to deploy the model automatically.\n",
    "\n",
    "Let's start by writing the Lambda function to take the model information and create a new hosting endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c75afd-d0d0-47f4-b7b6-9d590c5b600e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {CODE_FOLDER}/lambda.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker\")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    data_capture_percentage = event[\"data_capture_percentage\"]\n",
    "    data_capture_destination = event[\"data_capture_destination\"]\n",
    "    role = event[\"role\"]\n",
    "    \n",
    "    timestamp = time.strftime(\"%m%d%H%M%S\", time.localtime())\n",
    "    model_name = f\"penguins-model-{timestamp}\"\n",
    "    endpoint_config_name = f\"penguins-endpoint-config-{timestamp}\"\n",
    "\n",
    "    sagemaker.create_model(\n",
    "        ModelName=model_name, \n",
    "        ExecutionRoleArn=role, \n",
    "        Containers=[{\n",
    "            \"ModelPackageName\": model_package_arn\n",
    "        }] \n",
    "    )\n",
    "\n",
    "    sagemaker.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"ModelName\": model_name,\n",
    "                \"InstanceType\": \"ml.m5.large\",\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }\n",
    "        ],\n",
    "        DataCaptureConfig={\n",
    "            \"EnableCapture\": True,\n",
    "            \"InitialSamplingPercentage\": data_capture_percentage,\n",
    "            \"DestinationS3Uri\": data_capture_destination,\n",
    "            \"CaptureOptions\": [\n",
    "                {\n",
    "                    \"CaptureMode\": \"Input\"\n",
    "                },\n",
    "                {\n",
    "                    \"CaptureMode\": \"Output\"\n",
    "                },\n",
    "            ],\n",
    "            \"CaptureContentTypeHeader\": {\n",
    "                \"JsonContentTypes\": [\n",
    "                    \"application/json\",\n",
    "                    \"application/octect-stream\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    response = sagemaker.list_endpoints(NameContains=endpoint_name, MaxResults=1)\n",
    "\n",
    "    if len(response[\"Endpoints\"]) == 0:\n",
    "        # If the endpoint doesn't exist, let's create it.\n",
    "        sagemaker.create_endpoint(\n",
    "            EndpointName=endpoint_name, \n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "        )\n",
    "    else:\n",
    "        # If the endpoint already exist, let's update it with the\n",
    "        # new configuration.\n",
    "        sagemaker.update_endpoint(\n",
    "            EndpointName=endpoint_name, \n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\"Endpoint deployed successfully\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a4446-6db4-4515-bb1d-88ee21013bb2",
   "metadata": {},
   "source": [
    "We need to ensure our Lambda function has permission to interact with SageMaker, so let's create a new role and then create the lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1cca3-ce3f-48e4-93c6-0ce6363d00bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambda_role_name = \"lambda-pipeline-role\"\n",
    "\n",
    "policies = [\n",
    "    \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "]\n",
    "\n",
    "role_arn = None\n",
    "\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName = lambda_role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"Service\": \"lambda.amazonaws.com\"\n",
    "                    },\n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        }),\n",
    "        Description=\"Lambda Pipeline Role\"\n",
    "    )\n",
    "\n",
    "    role_arn = response[\"Role\"][\"Arn\"]\n",
    "\n",
    "    for p in policies:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=lambda_role_name,\n",
    "            PolicyArn=p\n",
    "        )\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    response = iam_client.get_role(RoleName=lambda_role_name)\n",
    "    role_arn = response[\"Role\"][\"Arn\"]\n",
    "\n",
    "deploy_lambda_fn = Lambda(\n",
    "    function_name=\"deploy_fn\",\n",
    "    execution_role_arn=role_arn,\n",
    "    script=str(CODE_FOLDER / \"lambda.py\"),\n",
    "    handler=\"lambda.lambda_handler\",\n",
    "    timeout=600,\n",
    "    session=pipeline_session\n",
    ")\n",
    "\n",
    "deploy_lambda_fn.upsert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dcc96-2890-43ff-a6cd-f2a9969adc52",
   "metadata": {},
   "source": [
    "## Step 6 - Setting up the Lambda Step\n",
    "\n",
    "Let's define the [LambdaStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.lambda_step.LambdaStep) that will run the function to deploy the model.\n",
    "\n",
    "We can use [Data Capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html) to record the inputs and outputs of the endpoint to use them later for monitoring the model. We'll enable Data Capture using the following settings:\n",
    "\n",
    "* `data_capture_percentage`: Represents the percentage of information that flows through the endpoint that we want to capture. For this example, we'll set that to 100%.\n",
    "* `data_capture_destination`: Specifies the S3 location where we want to store the captured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6eda89-70f3-49b4-8983-90f4db72fd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_capture_percentage = ParameterInteger(\n",
    "    name=\"data_capture_percentage\",\n",
    "    default_value=100,\n",
    ")\n",
    "\n",
    "data_capture_destination = ParameterString(\n",
    "    name=\"data_capture_destination\",\n",
    "    default_value=f\"{S3_LOCATION}/monitoring/data-capture\",\n",
    ")\n",
    "\n",
    "deploy_step = LambdaStep(\n",
    "    name=\"deploy\",\n",
    "    lambda_func=deploy_lambda_fn,\n",
    "    inputs={\n",
    "        \"model_package_arn\": register_model_step.properties.ModelPackageArn,\n",
    "        \"endpoint_name\": ENDPOINT,\n",
    "        \"data_capture_percentage\": data_capture_percentage,\n",
    "        \"data_capture_destination\": data_capture_destination,\n",
    "        \"role\": role,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe2299-be2a-46a7-809e-6174d44abddb",
   "metadata": {},
   "source": [
    "## Step 7 - Modifying the Condition Step\n",
    "\n",
    "We need to modify the [Condition Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition) to include the new Deploy Step we just created. If the condition succeeds, we will register and deploy the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacdde0-a65f-4225-8303-2b0106da1ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-accuracy\",\n",
    "    conditions=[condition_gte],\n",
    "    if_steps=[\n",
    "        register_model_step, deploy_step\n",
    "    ],\n",
    "    else_steps=[fail_step], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6d67c-6906-434d-9c41-17a4efeb38d2",
   "metadata": {},
   "source": [
    "## Step 8 - Setting up the Pipeline\n",
    "\n",
    "We can now define the SageMaker Pipeline and submit its definition to the SageMaker Pipelines service to create the pipeline if it doesn't exist or update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1969a5e-2ebf-474f-a275-ae0946c2fab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session5_pipeline = Pipeline(\n",
    "    name=\"penguins-session5\",\n",
    "    parameters=[\n",
    "        dataset_location,\n",
    "        accuracy_threshold,\n",
    "        data_capture_percentage,\n",
    "        data_capture_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step, \n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "session5_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56782b2-4b40-488b-8e4f-f8ecfa056b10",
   "metadata": {},
   "source": [
    "## Step 9 - Testing Custom Endpoint\n",
    "\n",
    "Let's now test the endpoint we deployed automatically with the pipeline. We will use the function to create a predictor with a JSON encoder and decoder. \n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#0066cc;\">Uncomment the <code style=\"background-color:#0066cc;\">%%script</code> cell magic line to execute this cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8465e-9f50-4e12-9b39-240f9a4b180c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "waiter = sagemaker_client.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(\n",
    "    EndpointName=ENDPOINT,\n",
    "    WaiterConfig={\n",
    "        \"Delay\": 10,\n",
    "        \"MaxAttempts\": 30\n",
    "    }\n",
    ")\n",
    "\n",
    "predictor = Predictor(endpoint_name=ENDPOINT, serializer=JSONSerializer(), deserializer=JSONDeserializer()) \n",
    "\n",
    "predictor.predict({\n",
    "    \"island\": \"Dream\",\n",
    "    \"culmen_length_mm\": 46.4,\n",
    "    \"culmen_depth_mm\": 18.6,\n",
    "    \"flipper_length_mm\": 190.0,\n",
    "    \"body_mass_g\": 3450.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729bdf6-59d1-47c6-9abb-43ed530b75e2",
   "metadata": {},
   "source": [
    "Let's now delete the endpoint.\n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#0066cc;\">Uncomment the <code style=\"background-color:#0066cc;\">%%script</code> cell magic line to execute this cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e851a-2416-4a0b-b8a1-c483cde3d776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0db72b-a41c-4f11-9407-6cd4a1d16699",
   "metadata": {},
   "source": [
    "# Running the Pipeline\n",
    "\n",
    "Uncomment the appropriate line to run that specific Session's pipeline. \n",
    "\n",
    "<div class=\"alert\" style=\"background-color:#6e420c; color: #fff\"><strong>Note:</strong> \n",
    "    Running the pipelines corresponding to Session 5 and Session 6 will deploy the model to an endpoint. Ensure you <strong><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-delete-resources.html\" style=\"color: #00A7E1\">delete this endpoint</a></strong> when you finish using it to prevent extra charges.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4efe32-dd1c-4968-a873-c1e4ad21ab00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# session2_pipeline.start()\n",
    "\n",
    "session3_pipeline.start()\n",
    "# session4_pipeline.start()\n",
    "\n",
    "# The following two pipelines deploy the model to an endpoint.\n",
    "# Ensure you delete the endpoint when you finish using it.\n",
    "# session5_pipeline.start()\n",
    "# session6_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626406be-9aa5-4900-b401-21c7885d8a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:325223348818:studio-lifecycle-config/packages",
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
